"use strict";(self.webpackChunknebula=self.webpackChunknebula||[]).push([[510],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return m}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=u(n),m=r,f=d["".concat(s,".").concat(m)]||d[m]||p[m]||o;return n?a.createElement(f,i(i({ref:t},c),{},{components:n})):a.createElement(f,i({ref:t},c))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var u=2;u<o;u++)i[u]=n[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7385:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return u},toc:function(){return c},default:function(){return d}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],l={layout:"default"},s="Data Formats",u={unversionedId:"configs/formats",id:"configs/formats",isDocsHomePage:!1,title:"Data Formats",description:"Nebula supports big range of data formats to run anaytics on, it includes but limited to: CSV/TSV, JSON, THRIFT, PARQUET, GOOGLE SHEET.",source:"@site/docs/configs/2-formats.md",sourceDirName:"configs",slug:"/configs/formats",permalink:"/docs/configs/formats",tags:[],version:"current",sidebarPosition:2,frontMatter:{layout:"default"},sidebar:"defaultSidebar",previous:{title:"Common Configs",permalink:"/docs/configs/common"},next:{title:"Connect Kafka",permalink:"/docs/configs/connect_kafka"}},c=[{value:"CSV/TSV",id:"csvtsv",children:[]},{value:"JSON",id:"json",children:[]},{value:"THRIFT",id:"thrift",children:[]},{value:"PARQUET",id:"parquet",children:[]},{value:"GOOGLE SHEET",id:"google-sheet",children:[]}],p={toc:c};function d(e){var t=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"data-formats"},"Data Formats"),(0,o.kt)("p",null,"Nebula supports big range of data formats to run anaytics on, it includes but limited to: CSV/TSV, JSON, THRIFT, PARQUET, GOOGLE SHEET.\nA few more formats to be supported soon: Protobuf and Microsoft Excel. Adding supports new format is purely based on user requests and popularity of the format itself.\nPlease let us know by logging an issue in the Github project."),(0,o.kt)("h2",{id:"csvtsv"},"CSV/TSV"),(0,o.kt)("p",null,"Very simple but widely used data formats, human readable bug not storage efficient.\nNebula implements a CSV reader by following RFC4180, you can find Nebula CSV reader from ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/varchar-io/nebula/blob/master/src/storage/CsvReader.h"},"this source"),"."),(0,o.kt)("p",null,"To add a CSV sourced table, you can just specify ",(0,o.kt)("inlineCode",{parentName:"p"},"format")," as ",(0,o.kt)("strong",{parentName:"p"},"csv"),", you can aslo add csv properties to change a few supported properties"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"hasHeader"),": it indiates if every csv file has first row as header. Default to true if not specified."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"delimiter"),": if the csv file(s) has different delimiter, such as TAB for TSV files. Default to comma if not specified.")),(0,o.kt)("p",null,"Here is an example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'tables:\n  table-name-1:\n    retention:\n      max-mb: 1000\n      max-hr: 24\n    schema: "ROW<id:bigint, name:string, value: double>"\n    data: s3\n    loader: Roll\n    source: s3://nebula/data/%7BDATE%7D/csv/\n    backup: s3://nebula/n116/\n    format: csv\n    csv:\n      hasHeader: true\n      delimiter: ","\n    time:\n      type: macro\n      pattern: daily\n')),(0,o.kt)("p",null,"This config add a table named ",(0,o.kt)("inlineCode",{parentName:"p"},"table-name-1"),", data ingested from s3 for upto 1 day, time specified by MACRO ",(0,o.kt)("inlineCode",{parentName:"p"},"DATE")," value.\nThe data files are in CSV file format separated by comma, every csv file has header in it. The data keeps rolling daily."),(0,o.kt)("h2",{id:"json"},"JSON"),(0,o.kt)("p",null,"JSON is such a common format in the world of services, easy to communicate and flexbile to change as it is essentially just a string.\nNebula supports JSON data regardless the source type is static data, HTTP endpoints or realtime streaming like Kafka."),(0,o.kt)("p",null,"Adding a table with source of JSON format is similar to CSV, and you can use ",(0,o.kt)("inlineCode",{parentName:"p"},"json")," config to customize properties:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"rowsField"),": which field in the JSON object to get row data payload. There are two special values:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"[ROOT]"),": the whole payload is an array, every item is a row object."),(0,o.kt)("li",{parentName:"ul"},'"": the whole payload is a single row object.'),(0,o.kt)("li",{parentName:"ul"},'"field": the field to read the array of row objects.'))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"columnsMap"),': this is a map of "column" name to "field path" within a row object. for example, if you have a payload like this')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},"{\n  a: 1\n  b: 2\n  c: [\n    {\n      x: 10\n      y: 11\n    },\n    {\n      x: 20\n      y: 21\n    }\n  ]\n}\n")),(0,o.kt)("p",null,"You can define a table to consume this payload as a table"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'tables:\n  table-name-2:\n    retention:\n      max-mb: 1000\n      max-hr: 24\n    schema: "ROW<id:int, value:int>"\n    data: HTTP\n    loader: Swap\n    source: "http://somewhere/api/request=what"\n    backup: s3://nebula/n120/\n    format: json\n    json:\n      rowsField: "c"\n      columnsMap:\n        id: "x"\n        value: "y"\n    time:\n      type: current\n')),(0,o.kt)("p",null,"This table config basically asks Nebula to load data from given HTTP URL, in the returned payload, we extract rows from field ",(0,o.kt)("inlineCode",{parentName:"p"},"c"),", and for each row, we get value of ",(0,o.kt)("inlineCode",{parentName:"p"},"x")," for column ",(0,o.kt)("inlineCode",{parentName:"p"},"id")," and value of ",(0,o.kt)("inlineCode",{parentName:"p"},"y")," for column ",(0,o.kt)("inlineCode",{parentName:"p"},"value"),"."),(0,o.kt)("h2",{id:"thrift"},"THRIFT"),(0,o.kt)("p",null,"Similar to JSON, thrift is another common binary format used to encode a message, which is smaller and faster than JSON.\nThrift data format has two properties you can set:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"protocol"),': "binary" is the only supported thrift serialization protocol (TBinaryProtocol). Could easily add support other types such as TCompatProtocol if needed.'),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"columnsMap"),": similar like JSON, this mapping is column name to field ID (unsigned integer).")),(0,o.kt)("p",null,"For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'tables:\n  k.table:\n    retention:\n      max-mb: 200000\n      max-hr: 12\n    schema: "ROW<userId:long, magicType:short, statusCode:byte, objectCount:int>"\n    data: kafka\n    topic: homefeed\n    loader: Roll\n    source: kafkabroker.home.01\n    backup: s3://nebula/n105/\n    format: thrift\n    thrift:\n      protocol: binary    \n      columnsMap:\n        # TODO(cao): this temporary hack to work around nested thrift definition\n        # we\'re using 1K to separate two levels asssuming no thrift definition has more than 1K fields\n        # in reverse order, such as 1003 => (field 3 -> field 1)\n        _time_: 1\n        userId: 3001\n        magicType: 3003\n        statusCode: 4002\n        objectCount: 4001\n')),(0,o.kt)("p",null,"We need some more work to support nested structure in thrift object, recommend users to try to flat your structure if possible."),(0,o.kt)("h2",{id:"parquet"},"PARQUET"),(0,o.kt)("h2",{id:"google-sheet"},"GOOGLE SHEET"))}d.isMDXComponent=!0}}]);